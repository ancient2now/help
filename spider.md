# 分布式爬虫

## 爬虫基础

### scrapy-redis

把 url 存在某个地方（redis），共享给所有的机器，总的调度器来分配请求，判断 spider 有没有闲置，闲置了就继续给它任务，直到所有的 url 都爬完，这种方法解决了去重问题，也能提高性能，scrapy-redis 就实现了这样一个完整框架，总的来说，这更适合广度优先的爬取。

### Headers设置

* User-Agent: 部分服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求
* Content-Type: 使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析
* application/xml: 在 XMl RPC, 如 RESTful/SOAP 调用时使用
* application/json: 在 JSON RPC 调用时使用
* application/x-www-form-urlencoded: 浏览器提交 Web 表单时使用

## 爬虫难点

### 反爬虫机制

A. 异常流量/高下载率 - 特别是时间内，从单一的短的客户端/或IP地址发送的请求。

B. 在网站上重复执行的任务 - 基于假设是人类用户将无法一直执行相同的重复任务。

C. 通过检测蜜罐 - 蜜罐通常是那些对于一个普通用户不可见，而只有爬虫可见的链接。当爬虫试图访问该链接时，警报就会触发。

**针对这些反爬虫机制，可以通过以下策略来避开或者绕过这样的封锁:**

#### A. 轮换IP地址

IP黑名单是可能的爬虫的最简单的方法。通过创建IP地址池，并使用不同的IP发出请求，将使服务器很难检测到爬虫。

对策：  
使用来自代理服务的IP列表，一定的时间间隔后随机挑选一个IP

#### B. 轮换Cookie

Cookies是加密存储在客户端的数据，有的网站使用Cookie来标识用户。如果用户在客户端发送高频请求，有可能被认定为可疑爬虫，从而拒绝访问。

对策：  
自定义和管理cookie池  
发送不包含cookie的请求到服务器，解析返回的包并设置cookie值; 它存储在cookie的收集器;  
从cookie收集器中获取cookie，如果cookie不可用，则从cookie收集器中将其删除;  
利用时间戳管理cookie收集器，这样是为了让爬虫每次首先获得的距离当前时间最远的cookie。  
关闭Cookies  
通过禁用cookie，可以帮助防止某些网站通过使用cookie来标识用户，从而导致爬虫被封禁。  

#### C. 用户代理欺骗

伪装成浏览器的方法之一是修改用户代理(User Agent)。用户代理是在请求Header中的字符串，它包含用户代理信息，例如网络浏览器，客户端，操作系统等的版本。

对策： 
通过用户代理的列表或者随机生成器，随机选择或生成对于每个请求的欺骗用户代理。设置为用户代理一个常见的网页浏览器，而不是使用默认的请求客户端。

#### D. 限制速度

降低抓取速度，善待网站，而不要让其不堪重负，或者DDoS攻击服务器。

对策：  
在每个请求之间的放入一些随机休眠时间
在抓取一定的网页数后，添加一些延迟
使用尽可能小的并发请求数目

#### E *避免重复性爬行模式

有些网站实现智能防抓取机制，从而重复操作将有可能检测为爬虫。为了让爬虫看起来像一个人，加上随机点击，鼠标移动，随机行为等。使用自动化测试工具，如可模拟器正常的“人的行为”。

#### F *小心蜜罐

这些蜜罐（HoneyPot）通常是链接，普通用户无法看到，但爬虫则可以。他们有可能在CSS样式中显示“display: none”。因此，蜜罐的检测可能会非常棘手。

*以上"反反爬虫"策略也可阅读博文Anti Anti-spider Strategy*